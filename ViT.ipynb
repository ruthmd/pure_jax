{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed5bf79-6c42-4a2d-b331-4e20fd6922da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.4.1 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.9.0.52 which is incompatible.\n",
      "tensorflow 2.17.0 requires ml-dtypes<0.5.0,>=0.3.1, but you have ml-dtypes 0.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow_datasets umap-learn \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.htmlimport jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df783193-bf34-4979-b0c8-44c8f6301868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. IMPORTS\n",
    "# ==============================\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, vmap\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Tuple, List, Any, NamedTuple\n",
    "\n",
    "# For visualization and metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100b354-72d1-4d12-a77d-b7ec9675949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 2. CONFIGURATION CLASSES\n",
    "# ==============================\n",
    "@dataclass\n",
    "class ViTConfig:\n",
    "    \"\"\"Configuration for Vision Transformer model\"\"\"\n",
    "    # Image processing\n",
    "    img_size: int = 32\n",
    "    patch_size: int = 4\n",
    "    \n",
    "    # Model architecture\n",
    "    num_classes: int = 10\n",
    "    num_heads: int = 8\n",
    "    num_layers: int = 6\n",
    "    hidden_dim: int = 384\n",
    "    mlp_dim: int = 1536\n",
    "    dropout_rate: float = 0.1\n",
    "    initializer_range: float = 0.02\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size: int = 128\n",
    "    num_epochs: int = 30\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 500\n",
    "    \n",
    "    # Optimizer parameters\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.999\n",
    "    eps: float = 1e-8\n",
    "    \n",
    "    # Other settings\n",
    "    seed: int = 42\n",
    "    \n",
    "    @property\n",
    "    def num_patches(self) -> int:\n",
    "        \"\"\"Calculate the number of patches based on image and patch size\"\"\"\n",
    "        return (self.img_size // self.patch_size) ** 2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingState:\n",
    "    \"\"\"State for tracking training progress\"\"\"\n",
    "    params: Any\n",
    "    opt_state: Any\n",
    "    train_losses: List[float] = field(default_factory=list)\n",
    "    train_accs: List[float] = field(default_factory=list)\n",
    "    eval_losses: List[float] = field(default_factory=list)\n",
    "    eval_accs: List[float] = field(default_factory=list)\n",
    "    step: int = 0\n",
    "    epoch: int = 0\n",
    "    best_accuracy: float = 0.0\n",
    "    best_epoch: int = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea6533-b130-4243-bab8-abd532377932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3. UTILITY CLASSES\n",
    "# ==============================\n",
    "class Utils:\n",
    "    \"\"\"Utility functions for the Vision Transformer\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_device(x):\n",
    "        \"\"\"Helper function to move data to the selected device\"\"\"\n",
    "        device = jax.devices()[0]\n",
    "        return jax.device_put(x, device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_initializer(scale: float = 0.02):\n",
    "        \"\"\"Get weight initializer function\"\"\"\n",
    "        return lambda key, shape, dtype=jnp.float32: random.normal(key, shape, dtype) * scale\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"Class for computing and tracking metrics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> float:\n",
    "        \"\"\"Compute cross-entropy loss\"\"\"\n",
    "        one_hot_labels = jax.nn.one_hot(labels, 10)  # CIFAR-10 has 10 classes\n",
    "        softmax_logits = jax.nn.log_softmax(logits)\n",
    "        loss = -jnp.sum(one_hot_labels * softmax_logits) / labels.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(logits: jnp.ndarray, labels: jnp.ndarray) -> float:\n",
    "        \"\"\"Compute accuracy\"\"\"\n",
    "        preds = jnp.argmax(logits, axis=-1)\n",
    "        return jnp.mean(preds == labels)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_predictions(logits: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Get class predictions from logits\"\"\"\n",
    "        return jnp.argmax(logits, axis=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_detailed_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
    "        \"\"\"Compute detailed metrics including confusion matrix\"\"\"\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Compute precision, recall, and F1 score for each class\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "        \n",
    "        # Compute macro-averaged metrics\n",
    "        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='macro'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'confusion_matrix': cm,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'macro_precision': macro_precision,\n",
    "            'macro_recall': macro_recall,\n",
    "            'macro_f1': macro_f1\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7e7c2d-eb4f-436b-82da-3c9bb1fc69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 4. PARAMETER INITIALIZATION\n",
    "# ==============================\n",
    "class ParameterInitializer:\n",
    "    \"\"\"Handles parameter initialization for the Vision Transformer\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ViTConfig):\n",
    "        self.config = config\n",
    "        self.device = jax.devices()[0]\n",
    "        \n",
    "    def init_linear_params(self, key: jnp.ndarray, in_dim: int, out_dim: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"Initialize linear layer parameters\"\"\"\n",
    "        k1, k2 = random.split(key)\n",
    "        weight_init = Utils.get_initializer(self.config.initializer_range)\n",
    "        weight = weight_init(k1, (in_dim, out_dim))\n",
    "        bias = jnp.zeros((out_dim,))\n",
    "        return weight, bias\n",
    "    \n",
    "    def init_layer_norm_params(self, dim: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"Initialize layer normalization parameters\"\"\"\n",
    "        scale = jnp.ones((dim,))\n",
    "        bias = jnp.zeros((dim,))\n",
    "        return scale, bias\n",
    "    \n",
    "    def init_transformer_params(self, key: jnp.ndarray) -> Dict:\n",
    "        \"\"\"Initialize all transformer parameters\"\"\"\n",
    "        keys = random.split(key, num=self.config.num_layers * 4 + 5)\n",
    "        key_idx = 0\n",
    "        \n",
    "        # Projection layer (patch embedding)\n",
    "        patch_projection_w, patch_projection_b = self.init_linear_params(\n",
    "            keys[key_idx], \n",
    "            self.config.patch_size * self.config.patch_size * 3, \n",
    "            self.config.hidden_dim\n",
    "        )\n",
    "        key_idx += 1\n",
    "        \n",
    "        # Class token and position embeddings\n",
    "        cls_token = random.normal(keys[key_idx], (1, self.config.hidden_dim)) * self.config.initializer_range\n",
    "        key_idx += 1\n",
    "        pos_embedding = random.normal(\n",
    "            keys[key_idx], \n",
    "            (1, self.config.num_patches + 1, self.config.hidden_dim)\n",
    "        ) * self.config.initializer_range\n",
    "        key_idx += 1\n",
    "        \n",
    "        # Initialize transformer blocks\n",
    "        encoder_blocks = []\n",
    "        for _ in range(self.config.num_layers):\n",
    "            block = self._init_transformer_block(keys[key_idx:key_idx+4])\n",
    "            encoder_blocks.append(block)\n",
    "            key_idx += 4\n",
    "        \n",
    "        # Final layer norm and classification head\n",
    "        ln_final_scale, ln_final_bias = self.init_layer_norm_params(self.config.hidden_dim)\n",
    "        head_w, head_b = self.init_linear_params(keys[key_idx], self.config.hidden_dim, self.config.num_classes)\n",
    "        \n",
    "        # Construct the parameter dictionary\n",
    "        params = {\n",
    "            'patch_projection_w': patch_projection_w,\n",
    "            'patch_projection_b': patch_projection_b,\n",
    "            'cls_token': cls_token,\n",
    "            'pos_embedding': pos_embedding,\n",
    "            'encoder_blocks': encoder_blocks,\n",
    "            'ln_final_scale': ln_final_scale,\n",
    "            'ln_final_bias': ln_final_bias,\n",
    "            'head_w': head_w,\n",
    "            'head_b': head_b,\n",
    "        }\n",
    "        \n",
    "        # Move all parameters to device\n",
    "        params = jax.tree_util.tree_map(lambda x: jax.device_put(x, self.device), params)\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _init_transformer_block(self, keys: List[jnp.ndarray]) -> Dict:\n",
    "        \"\"\"Initialize a single transformer block\"\"\"\n",
    "        # Layer norm 1\n",
    "        ln1_scale, ln1_bias = self.init_layer_norm_params(self.config.hidden_dim)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        qkv_w, qkv_b = self.init_linear_params(keys[0], self.config.hidden_dim, 3 * self.config.hidden_dim)\n",
    "        out_w, out_b = self.init_linear_params(keys[1], self.config.hidden_dim, self.config.hidden_dim)\n",
    "        \n",
    "        # Layer norm 2\n",
    "        ln2_scale, ln2_bias = self.init_layer_norm_params(self.config.hidden_dim)\n",
    "        \n",
    "        # MLP\n",
    "        mlp1_w, mlp1_b = self.init_linear_params(keys[2], self.config.hidden_dim, self.config.mlp_dim)\n",
    "        mlp2_w, mlp2_b = self.init_linear_params(keys[3], self.config.mlp_dim, self.config.hidden_dim)\n",
    "        \n",
    "        return {\n",
    "            'ln1_scale': ln1_scale,\n",
    "            'ln1_bias': ln1_bias,\n",
    "            'qkv_w': qkv_w,\n",
    "            'qkv_b': qkv_b,\n",
    "            'out_w': out_w,\n",
    "            'out_b': out_b,\n",
    "            'ln2_scale': ln2_scale,\n",
    "            'ln2_bias': ln2_bias,\n",
    "            'mlp1_w': mlp1_w,\n",
    "            'mlp1_b': mlp1_b,\n",
    "            'mlp2_w': mlp2_w,\n",
    "            'mlp2_b': mlp2_b,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4907f14-0cdb-4a0a-81ad-f4bfa75853dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# 5. MODEL COMPONENTS\n",
    "# ==============================\n",
    "class ModelLayers:\n",
    "    \"\"\"Basic neural network layers\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(x: jnp.ndarray, w: jnp.ndarray, b: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Linear layer\"\"\"\n",
    "        return jnp.dot(x, w) + b\n",
    "    \n",
    "    @staticmethod\n",
    "    def layer_norm(x: jnp.ndarray, scale: jnp.ndarray, bias: jnp.ndarray, eps: float = 1e-6) -> jnp.ndarray:\n",
    "        \"\"\"Layer normalization\"\"\"\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True)\n",
    "        return scale * (x - mean) / jnp.sqrt(var + eps) + bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def gelu(x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"GELU activation function\"\"\"\n",
    "        return 0.5 * x * (1 + jnp.tanh(jnp.sqrt(2 / jnp.pi) * (x + 0.044715 * x**3)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def dropout(key: jnp.ndarray, x: jnp.ndarray, rate: float) -> jnp.ndarray:\n",
    "        \"\"\"Apply dropout\"\"\"\n",
    "        if rate == 0.0:\n",
    "            return x\n",
    "        keep_prob = 1.0 - rate\n",
    "        mask = random.bernoulli(key, keep_prob, x.shape)\n",
    "        return x * mask / keep_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803535ff-4730-444a-89e2-3a0d7a565842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 6. TRANSFORMER COMPONENTS\n",
    "# ==============================\n",
    "class TransformerBlocks:\n",
    "    \"\"\"Transformer-specific components\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ViTConfig):\n",
    "        self.config = config\n",
    "        self.layers = ModelLayers()\n",
    "    \n",
    "    def multi_head_attention(\n",
    "        self, \n",
    "        x: jnp.ndarray, \n",
    "        params: Dict, \n",
    "        key: jnp.ndarray, \n",
    "        training: bool = True\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Multi-head self-attention\"\"\"\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        head_dim = hidden_dim // self.config.num_heads\n",
    "        \n",
    "        # Project queries, keys, and values\n",
    "        qkv = self.layers.linear(x, params['qkv_w'], params['qkv_b'])\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.config.num_heads, head_dim)\n",
    "        qkv = jnp.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scale = jnp.sqrt(head_dim)\n",
    "        attention_scores = jnp.matmul(q, jnp.swapaxes(k, -2, -1)) / scale\n",
    "        attention_probs = jax.nn.softmax(attention_scores)\n",
    "        \n",
    "        # Apply dropout to attention probabilities during training\n",
    "        if training and self.config.dropout_rate > 0:\n",
    "            attention_key = random.split(key, num=1)[0]\n",
    "            attention_probs = self.layers.dropout(attention_key, attention_probs, self.config.dropout_rate)\n",
    "        \n",
    "        context = jnp.matmul(attention_probs, v)\n",
    "        context = jnp.transpose(context, (0, 2, 1, 3))\n",
    "        context = context.reshape(batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.layers.linear(context, params['out_w'], params['out_b'])\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training and self.config.dropout_rate > 0:\n",
    "            output_key = random.split(key, num=1)[0]\n",
    "            output = self.layers.dropout(output_key, output, self.config.dropout_rate)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def mlp_block(\n",
    "        self, \n",
    "        x: jnp.ndarray, \n",
    "        params: Dict, \n",
    "        key: jnp.ndarray, \n",
    "        training: bool = True\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"MLP block with GELU activation\"\"\"\n",
    "        # First dense layer\n",
    "        x = self.layers.linear(x, params['mlp1_w'], params['mlp1_b'])\n",
    "        x = self.layers.gelu(x)\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training and self.config.dropout_rate > 0:\n",
    "            mlp1_key = random.split(key, num=1)[0]\n",
    "            x = self.layers.dropout(mlp1_key, x, self.config.dropout_rate)\n",
    "        \n",
    "        # Second dense layer\n",
    "        x = self.layers.linear(x, params['mlp2_w'], params['mlp2_b'])\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training and self.config.dropout_rate > 0:\n",
    "            mlp2_key = random.split(key, num=1)[0]\n",
    "            x = self.layers.dropout(mlp2_key, x, self.config.dropout_rate)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def encoder_block(\n",
    "        self, \n",
    "        x: jnp.ndarray, \n",
    "        params: Dict, \n",
    "        key: jnp.ndarray, \n",
    "        training: bool = True\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"Transformer encoder block\"\"\"\n",
    "        # Layer norm 1\n",
    "        norm1 = self.layers.layer_norm(x, params['ln1_scale'], params['ln1_bias'])\n",
    "        \n",
    "        # Multi-head attention with residual connection\n",
    "        attn_key = random.split(key, num=1)[0]\n",
    "        attn_output = self.multi_head_attention(norm1, params, attn_key, training)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # Layer norm 2\n",
    "        norm2 = self.layers.layer_norm(x, params['ln2_scale'], params['ln2_bias'])\n",
    "        \n",
    "        # MLP block with residual connection\n",
    "        mlp_key = random.split(key, num=1)[0]\n",
    "        mlp_output = self.mlp_block(norm2, params, mlp_key, training)\n",
    "        x = x + mlp_output\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b103ec-9280-4468-8648-128c27243972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 7. MAIN MODEL CLASS\n",
    "# ==============================\n",
    "class VisionTransformer:\n",
    "    \"\"\"Vision Transformer model implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ViTConfig):\n",
    "        self.config = config\n",
    "        self.layers = ModelLayers()\n",
    "        self.transformer = TransformerBlocks(config)\n",
    "    \n",
    "    def __call__(\n",
    "        self, \n",
    "        params: Dict, \n",
    "        images: jnp.ndarray, \n",
    "        key: jnp.ndarray, \n",
    "        training: bool = True,\n",
    "        return_features: bool = False\n",
    "    ) -> Any:\n",
    "        \"\"\"Forward pass of Vision Transformer\"\"\"\n",
    "        batch_size, height, width, channels = images.shape\n",
    "        \n",
    "        # Reshape images into patches\n",
    "        patches = jnp.reshape(\n",
    "            images, \n",
    "            (batch_size, height // self.config.patch_size, self.config.patch_size, \n",
    "             width // self.config.patch_size, self.config.patch_size, channels)\n",
    "        )\n",
    "        patches = jnp.transpose(patches, (0, 1, 3, 2, 4, 5))\n",
    "        patches = jnp.reshape(\n",
    "            patches, \n",
    "            (batch_size, self.config.num_patches, self.config.patch_size * self.config.patch_size * channels)\n",
    "        )\n",
    "        \n",
    "        # Linear projection of flattened patches\n",
    "        patch_embeddings = self.layers.linear(\n",
    "            patches, \n",
    "            params['patch_projection_w'], \n",
    "            params['patch_projection_b']\n",
    "        )\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = jnp.broadcast_to(\n",
    "            params['cls_token'], \n",
    "            (batch_size, 1, self.config.hidden_dim)\n",
    "        )\n",
    "        x = jnp.concatenate([cls_tokens, patch_embeddings], axis=1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + params['pos_embedding']\n",
    "        \n",
    "        # Apply dropout to embeddings during training\n",
    "        if training and self.config.dropout_rate > 0:\n",
    "            embed_key = random.split(key, num=1)[0]\n",
    "            x = self.layers.dropout(embed_key, x, self.config.dropout_rate)\n",
    "        \n",
    "        # Apply transformer encoder blocks\n",
    "        for i, block_params in enumerate(params['encoder_blocks']):\n",
    "            block_key = random.fold_in(key, i)\n",
    "            x = self.transformer.encoder_block(x, block_params, block_key, training)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        x = self.layers.layer_norm(x, params['ln_final_scale'], params['ln_final_bias'])\n",
    "        \n",
    "        # Use [CLS] token representation for classification\n",
    "        cls_representation = x[:, 0]\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.layers.linear(cls_representation, params['head_w'], params['head_b'])\n",
    "        \n",
    "        if return_features:\n",
    "            return logits, cls_representation\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561da002-f1a2-4350-8fd3-4ada8d0ca424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 8. OPTIMIZER\n",
    "# ==============================\n",
    "class AdamWOptimizer:\n",
    "    \"\"\"AdamW optimizer implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ViTConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def init_state(self, params: Dict) -> Dict:\n",
    "        \"\"\"Initialize optimizer state\"\"\"\n",
    "        # Initialize first and second moments to zeros\n",
    "        m = jax.tree_util.tree_map(jnp.zeros_like, params)\n",
    "        v = jax.tree_util.tree_map(jnp.zeros_like, params)\n",
    "        \n",
    "        return {\n",
    "            'm': m,\n",
    "            'v': v,\n",
    "            't': 0,\n",
    "        }\n",
    "    \n",
    "    def update(\n",
    "        self, \n",
    "        grads: Dict, \n",
    "        opt_state: Dict, \n",
    "        params: Dict, \n",
    "        learning_rate: float\n",
    "    ) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Apply AdamW update step\"\"\"\n",
    "        # Extract optimizer state\n",
    "        m, v, t = opt_state['m'], opt_state['v'], opt_state['t']\n",
    "        \n",
    "        # Increment step\n",
    "        t = t + 1\n",
    "        \n",
    "        # Compute bias correction terms\n",
    "        bias_correction1 = 1 - self.config.beta1 ** t\n",
    "        bias_correction2 = 1 - self.config.beta2 ** t\n",
    "        \n",
    "        # Define update function for a single parameter pair\n",
    "        def update_momentum(m_param, grad):\n",
    "            return self.config.beta1 * m_param + (1 - self.config.beta1) * grad\n",
    "        \n",
    "        def update_velocity(v_param, grad):\n",
    "            return self.config.beta2 * v_param + (1 - self.config.beta2) * jnp.square(grad)\n",
    "        \n",
    "        def update_param(param, m_param, v_param):\n",
    "            # Compute bias-corrected moment estimates\n",
    "            m_hat = m_param / bias_correction1\n",
    "            v_hat = v_param / bias_correction2\n",
    "            \n",
    "            # Apply weight decay\n",
    "            param_with_decay = param - learning_rate * self.config.weight_decay * param\n",
    "            \n",
    "            # Update parameter\n",
    "            return param_with_decay - learning_rate * m_hat / (jnp.sqrt(v_hat) + self.config.eps)\n",
    "        \n",
    "        # Update momentum terms\n",
    "        new_m = jax.tree_util.tree_map(update_momentum, m, grads)\n",
    "        \n",
    "        # Update velocity terms  \n",
    "        new_v = jax.tree_util.tree_map(update_velocity, v, grads)\n",
    "        \n",
    "        # Update parameters\n",
    "        new_params = jax.tree_util.tree_map(update_param, params, new_m, new_v)\n",
    "        \n",
    "        # Update optimizer state\n",
    "        new_opt_state = {\n",
    "            'm': new_m,\n",
    "            'v': new_v,\n",
    "            't': t,\n",
    "        }\n",
    "        \n",
    "        return new_params, new_opt_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014af4b-4e13-4ba0-a8cb-6e1e25e2235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# 9. LEARNING RATE SCHEDULER\n",
    "# ==============================\n",
    "class LearningRateScheduler:\n",
    "    \"\"\"Learning rate scheduler with warmup and cosine decay\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ViTConfig, total_steps: int):\n",
    "        self.config = config\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = config.warmup_steps\n",
    "        self.peak_lr = config.learning_rate\n",
    "        self.end_lr = 0.0\n",
    "        \n",
    "    def __call__(self, step: int) -> float:\n",
    "        \"\"\"Get learning rate for the current step\"\"\"\n",
    "        # Linear warmup\n",
    "        warmup_lr = self.peak_lr * jnp.minimum(1.0, step / self.warmup_steps)\n",
    "        \n",
    "        # Cosine decay\n",
    "        decay_steps = self.total_steps - self.warmup_steps\n",
    "        decay_factor = 0.5 * (1 + jnp.cos(\n",
    "            jnp.pi * jnp.minimum(step - self.warmup_steps, decay_steps) / decay_steps\n",
    "        ))\n",
    "        cosine_lr = self.end_lr + (self.peak_lr - self.end_lr) * decay_factor\n",
    "        \n",
    "        # Use warmup for steps < warmup_steps, cosine decay afterward\n",
    "        lr = jnp.where(step < self.warmup_steps, warmup_lr, cosine_lr)\n",
    "        \n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb83f4b-1eb9-423f-8df6-7baa4bb77594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 10. DATA LOADER\n",
    "# ==============================\n",
    "class DataLoader:\n",
    "    \"\"\"Handles data loading for CIFAR-10\"\"\"\n",
    "    \n",
    "    CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                       'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"cifar10_data\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "        \n",
    "    def load_datasets(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load CIFAR-10 dataset\"\"\"\n",
    "        # Create data directory\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        \n",
    "        # Download if necessary\n",
    "        tar_file_path = os.path.join(self.data_dir, \"cifar-10-python.tar.gz\")\n",
    "        if not os.path.exists(tar_file_path):\n",
    "            print(f\"Downloading CIFAR-10 dataset from {self.url}\")\n",
    "            self._download_dataset(tar_file_path)\n",
    "        \n",
    "        # Extract if necessary\n",
    "        extract_dir = os.path.join(self.data_dir, \"cifar-10-batches-py\")\n",
    "        if not os.path.exists(extract_dir):\n",
    "            print(f\"Extracting dataset to {extract_dir}\")\n",
    "            self._extract_dataset(tar_file_path, self.data_dir)\n",
    "        \n",
    "        # Load training and test data\n",
    "        train_data, train_labels = self._load_training_data(extract_dir)\n",
    "        test_data, test_labels = self._load_test_data(extract_dir)\n",
    "        \n",
    "        # Preprocess data\n",
    "        train_data = self._preprocess_data(train_data)\n",
    "        test_data = self._preprocess_data(test_data)\n",
    "        \n",
    "        print(f\"Loaded {len(train_data)} training samples and {len(test_data)} test samples\")\n",
    "        \n",
    "        return train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "    def _download_dataset(self, file_path: str):\n",
    "        \"\"\"Download CIFAR-10 dataset\"\"\"\n",
    "        urllib.request.urlretrieve(self.url, file_path)\n",
    "        print(f\"Dataset downloaded to {file_path}\")\n",
    "    \n",
    "    def _extract_dataset(self, tar_file_path: str, extract_to: str):\n",
    "        \"\"\"Extract CIFAR-10 dataset\"\"\"\n",
    "        with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=extract_to)\n",
    "        print(\"Dataset extracted\")\n",
    "    \n",
    "    def _load_training_data(self, extract_dir: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load training data from multiple batch files\"\"\"\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        \n",
    "        # Load training batches\n",
    "        for i in range(1, 6):\n",
    "            batch_file = os.path.join(extract_dir, f'data_batch_{i}')\n",
    "            with open(batch_file, 'rb') as f:\n",
    "                batch_data = pickle.load(f, encoding='bytes')\n",
    "                train_data.append(batch_data[b'data'])\n",
    "                train_labels.extend(batch_data[b'labels'])\n",
    "        \n",
    "        # Concatenate all training data\n",
    "        train_data = np.vstack(train_data)\n",
    "        train_labels = np.array(train_labels)\n",
    "        \n",
    "        return train_data, train_labels\n",
    "    \n",
    "    def _load_test_data(self, extract_dir: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load test data\"\"\"\n",
    "        test_file = os.path.join(extract_dir, 'test_batch')\n",
    "        with open(test_file, 'rb') as f:\n",
    "            test_batch = pickle.load(f, encoding='bytes')\n",
    "            test_data = test_batch[b'data']\n",
    "            test_labels = np.array(test_batch[b'labels'])\n",
    "        \n",
    "        return test_data, test_labels\n",
    "    \n",
    "    def _preprocess_data(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess CIFAR-10 data\"\"\"\n",
    "        # Reshape and normalize the data\n",
    "        # CIFAR-10 data layout: [N, 3072] where 3072 = 3 x 32 x 32\n",
    "        # We reshape to [N, 32, 32, 3] for image processing\n",
    "        data = data.reshape(-1, 3, 32, 32)\n",
    "        data = data.transpose(0, 2, 3, 1)\n",
    "        data = data.astype(np.float32) / 255.0\n",
    "        \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115655bc-fb36-4811-b6ba-097a3a15dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 11. VISUALIZER\n",
    "# ==============================\n",
    "class Visualizer:\n",
    "    \"\"\"Handles all visualization tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, results_dir: str):\n",
    "        self.results_dir = results_dir\n",
    "        self.cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    def plot_training_metrics(self, state: TrainingState):\n",
    "        \"\"\"Plot training and validation metrics over epochs\"\"\"\n",
    "        epochs = range(1, len(state.train_losses) + 1)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(epochs, state.train_losses, 'b-', label='Training Loss')\n",
    "        ax1.plot(epochs, state.eval_losses, 'r-', label='Validation Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(epochs, state.train_accs, 'b-', label='Training Accuracy')\n",
    "        ax2.plot(epochs, state.eval_accs, 'r-', label='Validation Accuracy')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.results_dir, 'training_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_learning_rate_schedule(self, lr_scheduler, total_steps: int):\n",
    "        \"\"\"Plot the learning rate schedule\"\"\"\n",
    "        steps = range(total_steps)\n",
    "        lrs = [float(lr_scheduler(step)) for step in steps]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(steps, lrs)\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(self.results_dir, 'lr_schedule.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm: np.ndarray):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=self.cifar10_classes, \n",
    "                    yticklabels=self.cifar10_classes)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.results_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_classification_metrics(self, metrics: Dict):\n",
    "        \"\"\"Plot precision, recall, and F1 scores\"\"\"\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1 Score': metrics['f1']\n",
    "        }, index=self.cifar10_classes)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        metrics_df.plot(kind='bar', figsize=(12, 6))\n",
    "        plt.title('Precision, Recall, and F1 Score by Class')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.tight_layout()\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(os.path.join(self.results_dir, 'precision_recall_f1.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_feature_space(self, features: np.ndarray, labels: np.ndarray, subset_size: int = 2000):\n",
    "        \"\"\"Visualize feature space using t-SNE, PCA, and UMAP\"\"\"\n",
    "        # Use subset if needed\n",
    "        if len(features) > subset_size:\n",
    "            subset_indices = np.random.choice(len(features), subset_size, replace=False)\n",
    "            features = features[subset_indices]\n",
    "            labels = labels[subset_indices]\n",
    "        \n",
    "        # t-SNE visualization\n",
    "        print(\"Computing t-SNE projection...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, n_jobs=-1)\n",
    "        tsne_result = tsne.fit_transform(features)\n",
    "        \n",
    "        self._plot_2d_embedding(tsne_result, labels, 't-SNE Visualization of Feature Space', 'tsne_visualization.png')\n",
    "        \n",
    "        # PCA visualization\n",
    "        print(\"Computing PCA projection...\")\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        pca_result = pca.fit_transform(features)\n",
    "        \n",
    "        self._plot_2d_embedding(pca_result, labels, 'PCA Visualization of Feature Space', 'pca_visualization.png')\n",
    "        \n",
    "        # UMAP visualization\n",
    "        print(\"Computing UMAP projection...\")\n",
    "        umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "        umap_result = umap_model.fit_transform(features)\n",
    "        \n",
    "        self._plot_2d_embedding(umap_result, labels, 'UMAP Visualization of Feature Space', 'umap_visualization.png')\n",
    "    \n",
    "    def _plot_2d_embedding(self, embedding: np.ndarray, labels: np.ndarray, title: str, filename: str):\n",
    "        \"\"\"Plot 2D embedding with class colors\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create scatter plot for each class\n",
    "        for i, class_name in enumerate(self.cifar10_classes):\n",
    "            indices = labels == i\n",
    "            plt.scatter(embedding[indices, 0], embedding[indices, 1], \n",
    "                       label=class_name, alpha=0.7, s=20)\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.results_dir, filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_sample_predictions(\n",
    "        self, \n",
    "        vit: VisionTransformer,\n",
    "        params: Dict,\n",
    "        test_data: np.ndarray,\n",
    "        test_labels: np.ndarray,\n",
    "        num_samples: int = 10\n",
    "    ):\n",
    "        \"\"\"Visualize sample predictions\"\"\"\n",
    "        # Get random subset of test samples\n",
    "        indices = np.random.choice(len(test_data), num_samples, replace=False)\n",
    "        images = test_data[indices]\n",
    "        labels = test_labels[indices]\n",
    "        \n",
    "        # Move to device and get predictions\n",
    "        device = jax.devices()[0]\n",
    "        images_device = jax.device_put(images, device)\n",
    "        labels_device = jax.device_put(labels, device)\n",
    "        \n",
    "        # Get predictions\n",
    "        eval_key = jax.device_put(random.PRNGKey(0), device)\n",
    "        logits = vit(params, images_device, eval_key, training=False)\n",
    "        preds = Metrics.get_predictions(logits)\n",
    "        \n",
    "        # Move back to CPU for visualization\n",
    "        images_cpu = np.array(images)\n",
    "        preds_cpu = np.array(preds)\n",
    "        labels_cpu = np.array(labels)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (image, label, pred) in enumerate(zip(images_cpu, labels_cpu, preds_cpu)):\n",
    "            ax = axes[i]\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f\"True: {self.cifar10_classes[label]}\\nPred: {self.cifar10_classes[pred]}\")\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Highlight correct/incorrect predictions\n",
    "            if label == pred:\n",
    "                # Green border for correct predictions\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_edgecolor('green')\n",
    "                    spine.set_linewidth(3)\n",
    "                    spine.set_visible(True)\n",
    "            else:\n",
    "                # Red border for incorrect predictions\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_edgecolor('red')\n",
    "                    spine.set_linewidth(3)\n",
    "                    spine.set_visible(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.results_dir, 'sample_predictions.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2053d8-b3de-4edf-8fe8-c28981056f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 12. TRAINER CLASS\n",
    "# ==============================\n",
    "class Trainer:\n",
    "    \"\"\"Handles the training loop and evaluation for Vision Transformer\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ViTConfig, results_dir: str = \"vit_results\"):\n",
    "        self.config = config\n",
    "        self.results_dir = results_dir\n",
    "        self.device = jax.devices()[0]\n",
    "        \n",
    "        # Initialize components\n",
    "        self.vit = VisionTransformer(config)\n",
    "        self.optimizer = AdamWOptimizer(config)\n",
    "        self.initializer = ParameterInitializer(config)\n",
    "        \n",
    "        # Set up JAX key\n",
    "        self.key = jax.device_put(random.PRNGKey(config.seed), self.device)\n",
    "        \n",
    "        # Initialize model and optimizer\n",
    "        init_key, self.key = random.split(self.key)\n",
    "        self.params = self.initializer.init_transformer_params(init_key)\n",
    "        self.opt_state = self.optimizer.init_state(self.params)\n",
    "        \n",
    "        # Initialize learning rate scheduler (will be set in train method)\n",
    "        self.lr_scheduler = None\n",
    "        \n",
    "        # Initialize state tracking\n",
    "        self.state = TrainingState(\n",
    "            params=self.params,\n",
    "            opt_state=self.opt_state\n",
    "        )\n",
    "        \n",
    "        # Create JIT-compiled functions\n",
    "        self._compile_functions()\n",
    "    \n",
    "    def _compile_functions(self):\n",
    "        \"\"\"Compile JAX functions for better performance\"\"\"\n",
    "        self.jit_train_step = jax.jit(self._train_step)\n",
    "        self.jit_eval_step = jax.jit(self._eval_step)\n",
    "    \n",
    "    def _train_step(\n",
    "        self, \n",
    "        params: Dict, \n",
    "        batch: Tuple[jnp.ndarray, jnp.ndarray], \n",
    "        dropout_key: jnp.ndarray, \n",
    "        opt_state: Dict, \n",
    "        learning_rate: float\n",
    "    ) -> Tuple[Dict, float, float, Dict]:\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        images, labels = batch\n",
    "        images = jax.device_put(images, self.device)\n",
    "        labels = jax.device_put(labels, self.device)\n",
    "        \n",
    "        def loss_fn(params_weights):\n",
    "            logits = self.vit(params_weights, images, dropout_key, training=True)\n",
    "            loss = Metrics.cross_entropy_loss(logits, labels)\n",
    "            return loss, logits\n",
    "        \n",
    "        (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "        \n",
    "        # Apply optimizer\n",
    "        new_params, new_opt_state = self.optimizer.update(\n",
    "            grads, opt_state, params, learning_rate\n",
    "        )\n",
    "        \n",
    "        acc = Metrics.accuracy(logits, labels)\n",
    "        \n",
    "        return new_params, loss, acc, new_opt_state\n",
    "    \n",
    "    def _eval_step(\n",
    "        self, \n",
    "        params: Dict, \n",
    "        batch: Tuple[jnp.ndarray, jnp.ndarray], \n",
    "        return_features: bool = False\n",
    "    ) -> Any:\n",
    "        \"\"\"Single evaluation step\"\"\"\n",
    "        images, labels = batch\n",
    "        images = jax.device_put(images, self.device)\n",
    "        labels = jax.device_put(labels, self.device)\n",
    "        \n",
    "        # No dropout during evaluation\n",
    "        eval_key = jax.device_put(random.PRNGKey(0), self.device)\n",
    "        \n",
    "        if return_features:\n",
    "            logits, features = self.vit(params, images, eval_key, training=False, return_features=True)\n",
    "        else:\n",
    "            logits = self.vit(params, images, eval_key, training=False)\n",
    "        \n",
    "        loss = Metrics.cross_entropy_loss(logits, labels)\n",
    "        acc = Metrics.accuracy(logits, labels)\n",
    "        preds = Metrics.get_predictions(logits)\n",
    "        \n",
    "        if return_features:\n",
    "            return loss, acc, preds, features\n",
    "        else:\n",
    "            return loss, acc, preds\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        train_data: np.ndarray, \n",
    "        train_labels: np.ndarray, \n",
    "        test_data: np.ndarray, \n",
    "        test_labels: np.ndarray\n",
    "    ) -> Dict:\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        # Initialize learning rate scheduler\n",
    "        total_steps = self.config.num_epochs * (len(train_data) // self.config.batch_size)\n",
    "        self.lr_scheduler = LearningRateScheduler(self.config, total_steps)\n",
    "        \n",
    "        # Create results directory\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Starting training on {self.device}\")\n",
    "        print(f\"Total steps: {total_steps}\")\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            self.state.epoch = epoch\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # Training epoch\n",
    "            self._train_epoch(train_data, train_labels)\n",
    "            \n",
    "            # Evaluation epoch\n",
    "            eval_metrics = self._eval_epoch(test_data, test_labels)\n",
    "            \n",
    "            # Update state\n",
    "            self.state.train_losses.append(np.mean(self.epoch_train_losses))\n",
    "            self.state.train_accs.append(np.mean(self.epoch_train_accs))\n",
    "            self.state.eval_losses.append(eval_metrics['loss'])\n",
    "            self.state.eval_accs.append(eval_metrics['accuracy'])\n",
    "            \n",
    "            # Check for best model\n",
    "            if eval_metrics['accuracy'] > self.state.best_accuracy:\n",
    "                self.state.best_accuracy = eval_metrics['accuracy']\n",
    "                self.state.best_epoch = epoch\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{self.config.num_epochs} completed in {epoch_time:.2f}s\")\n",
    "            print(f\"  Train Loss: {self.state.train_losses[-1]:.4f}, Train Accuracy: {self.state.train_accs[-1]:.4f}\")\n",
    "            print(f\"  Eval Loss: {self.state.eval_losses[-1]:.4f}, Eval Accuracy: {self.state.eval_accs[-1]:.4f}\")\n",
    "            print(f\"  Best Accuracy: {self.state.best_accuracy:.4f} at epoch {self.state.best_epoch+1}\")\n",
    "            \n",
    "            # Save progress plots periodically\n",
    "            if (epoch + 1) % 5 == 0 or epoch == self.config.num_epochs - 1:\n",
    "                viz = Visualizer(self.results_dir)\n",
    "                viz.plot_training_metrics(self.state)\n",
    "        \n",
    "        # Final evaluation with features\n",
    "        final_metrics = self._final_evaluation(test_data, test_labels)\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    def _train_epoch(self, train_data: np.ndarray, train_labels: np.ndarray):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.epoch_train_losses = []\n",
    "        self.epoch_train_accs = []\n",
    "        \n",
    "        # Create batch iterator with progress bar\n",
    "        num_batches = len(train_data) // self.config.batch_size\n",
    "        batches = self._iterate_batches(train_data, train_labels, shuffle=True)\n",
    "        progress_bar = tqdm(batches, total=num_batches, \n",
    "                           desc=f\"Epoch {self.state.epoch+1}/{self.config.num_epochs} [Train]\",\n",
    "                           leave=True)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Generate new dropout key\n",
    "            dropout_key, self.key = random.split(self.key)\n",
    "            dropout_key = jax.device_put(dropout_key, self.device)\n",
    "            \n",
    "            # Get current learning rate\n",
    "            current_lr = float(self.lr_scheduler(self.state.step))\n",
    "            \n",
    "            # Update parameters\n",
    "            self.state.params, loss, acc, self.state.opt_state = self.jit_train_step(\n",
    "                self.state.params, batch, dropout_key, self.state.opt_state, current_lr\n",
    "            )\n",
    "            \n",
    "            self.epoch_train_losses.append(float(loss))\n",
    "            self.epoch_train_accs.append(float(acc))\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{float(loss):.4f}\",\n",
    "                'acc': f\"{float(acc):.4f}\",\n",
    "                'lr': f\"{current_lr:.6f}\"\n",
    "            })\n",
    "            \n",
    "            self.state.step += 1\n",
    "    \n",
    "    def _eval_epoch(self, test_data: np.ndarray, test_labels: np.ndarray) -> Dict:\n",
    "        \"\"\"Evaluate for one epoch\"\"\"\n",
    "        epoch_eval_losses = []\n",
    "        epoch_eval_accs = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Create test batch iterator with progress bar\n",
    "        num_test_batches = len(test_data) // self.config.batch_size\n",
    "        if len(test_data) % self.config.batch_size != 0:\n",
    "            num_test_batches += 1\n",
    "            \n",
    "        batches = self._iterate_batches(test_data, test_labels, shuffle=False)\n",
    "        progress_bar = tqdm(batches, total=num_test_batches, \n",
    "                           desc=f\"Epoch {self.state.epoch+1}/{self.config.num_epochs} [Eval]\",\n",
    "                           leave=True)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            loss, acc, preds = self.jit_eval_step(self.state.params, batch)\n",
    "            epoch_eval_losses.append(float(loss))\n",
    "            epoch_eval_accs.append(float(acc))\n",
    "            \n",
    "            # Collect predictions and labels\n",
    "            all_preds.append(np.array(preds))\n",
    "            all_labels.append(batch[1])\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{float(loss):.4f}\",\n",
    "                'acc': f\"{float(acc):.4f}\"\n",
    "            })\n",
    "        \n",
    "        # Compute average metrics\n",
    "        return {\n",
    "            'loss': np.mean(epoch_eval_losses),\n",
    "            'accuracy': np.mean(epoch_eval_accs),\n",
    "            'predictions': np.concatenate(all_preds),\n",
    "            'labels': np.concatenate(all_labels)\n",
    "        }\n",
    "    \n",
    "    def _final_evaluation(self, test_data: np.ndarray, test_labels: np.ndarray) -> Dict:\n",
    "        \"\"\"Final comprehensive evaluation\"\"\"\n",
    "        print(\"Running final evaluation with feature extraction...\")\n",
    "        \n",
    "        all_features = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Create progress bar for final evaluation\n",
    "        num_test_batches = len(test_data) // self.config.batch_size\n",
    "        if len(test_data) % self.config.batch_size != 0:\n",
    "            num_test_batches += 1\n",
    "            \n",
    "        batches = self._iterate_batches(test_data, test_labels, shuffle=False)\n",
    "        progress_bar = tqdm(batches, total=num_test_batches, \n",
    "                           desc=\"Final Evaluation\",\n",
    "                           leave=True)\n",
    "        \n",
    "        # Collect features and predictions\n",
    "        for batch in progress_bar:\n",
    "            loss, acc, preds, features = self._eval_step(self.state.params, batch, return_features=True)\n",
    "            all_features.append(np.array(features))\n",
    "            all_preds.append(np.array(preds))\n",
    "            all_labels.append(np.array(batch[1]))\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{float(loss):.4f}\",\n",
    "                'acc': f\"{float(acc):.4f}\"\n",
    "            })\n",
    "        \n",
    "        # Concatenate all results\n",
    "        features = np.concatenate(all_features)\n",
    "        predictions = np.concatenate(all_preds)\n",
    "        labels = np.concatenate(all_labels)\n",
    "        \n",
    "        # Compute detailed metrics\n",
    "        metrics = Metrics.compute_detailed_metrics(labels, predictions)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        viz = Visualizer(self.results_dir)\n",
    "        viz.plot_confusion_matrix(metrics['confusion_matrix'])\n",
    "        viz.plot_classification_metrics(metrics)\n",
    "        viz.plot_feature_space(features, labels, subset_size=2000)\n",
    "        \n",
    "        return {\n",
    "            'metrics': metrics,\n",
    "            'features': features,\n",
    "            'predictions': predictions,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    def _iterate_batches(\n",
    "        self, \n",
    "        images: np.ndarray, \n",
    "        labels: np.ndarray, \n",
    "        shuffle: bool = False\n",
    "    ):\n",
    "        \"\"\"Iterator for creating batches\"\"\"\n",
    "        num_samples = len(images)\n",
    "        indices = np.arange(num_samples)\n",
    "        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, num_samples, self.config.batch_size):\n",
    "            end_idx = min(start_idx + self.config.batch_size, num_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            yield images[batch_indices], labels[batch_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd18f8d-55d5-406d-ac79-7f4ab8305e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VISION TRANSFORMER TRAINING\n",
      "============================================================\n",
      "Configuration:\n",
      "  - Model: 6 layers × 384 dim\n",
      "  - Training: 30 epochs × 128 batch size\n",
      "  - Learning rate: 0.0003 (warmup: 500 steps)\n",
      "  - Device: cuda:0\n",
      "  - Results: vit_results/run_20250506_204653\n",
      "============================================================\n",
      "\n",
      "Loading CIFAR-10 dataset...\n",
      "Loaded 50000 training samples and 10000 test samples\n",
      "Creating visualizations...\n",
      "\n",
      "Training model...\n",
      "Starting training on cuda:0\n",
      "Total steps: 11700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5880a0ee60b48b181225e718abdb53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8aa1285265b4257bd88f5f0ecaa369d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 completed in 317.00s\n",
      "  Train Loss: 1.8906, Train Accuracy: 0.2865\n",
      "  Eval Loss: 1.5669, Eval Accuracy: 0.4156\n",
      "  Best Accuracy: 0.4156 at epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d576659fc254d229cdddff090b60558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb009fbbe514f1b878a06e9782577e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 completed in 7.58s\n",
      "  Train Loss: 1.4729, Train Accuracy: 0.4619\n",
      "  Eval Loss: 1.3942, Eval Accuracy: 0.4877\n",
      "  Best Accuracy: 0.4877 at epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42fdae5bd764aa7a00e14d0da7fe272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49225b552138428fafba7281217cc5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 completed in 7.60s\n",
      "  Train Loss: 1.2997, Train Accuracy: 0.5307\n",
      "  Eval Loss: 1.2964, Eval Accuracy: 0.5303\n",
      "  Best Accuracy: 0.5303 at epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb141fbd233d425bad41f04448e0d7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72922d5a9f31412697404f946ebbbb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 completed in 7.66s\n",
      "  Train Loss: 1.2284, Train Accuracy: 0.5565\n",
      "  Eval Loss: 1.2476, Eval Accuracy: 0.5485\n",
      "  Best Accuracy: 0.5485 at epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55882e137aa44cfeb03b48de9ca5c80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5088efb94a430d9f6df577a2d3b03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 completed in 7.65s\n",
      "  Train Loss: 1.1712, Train Accuracy: 0.5793\n",
      "  Eval Loss: 1.2490, Eval Accuracy: 0.5521\n",
      "  Best Accuracy: 0.5521 at epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d84c195cbd64e6a998fbdac062577ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2488ee7232fe4be7aa49fe488a56e474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 completed in 7.65s\n",
      "  Train Loss: 1.1309, Train Accuracy: 0.5925\n",
      "  Eval Loss: 1.1386, Eval Accuracy: 0.5877\n",
      "  Best Accuracy: 0.5877 at epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42952fffcb6d4a478805e42875181cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce5822d6b9242cea6369b11c95db4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 completed in 7.69s\n",
      "  Train Loss: 1.0873, Train Accuracy: 0.6106\n",
      "  Eval Loss: 1.1367, Eval Accuracy: 0.5965\n",
      "  Best Accuracy: 0.5965 at epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43c5bffda424c53835705c471c04720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cdcb4634f04b94b81488985d38f2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 completed in 7.68s\n",
      "  Train Loss: 1.0537, Train Accuracy: 0.6237\n",
      "  Eval Loss: 1.0884, Eval Accuracy: 0.6097\n",
      "  Best Accuracy: 0.6097 at epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd2e14c41c145ab9069453bce04cc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b34b074943443a8419e840975d02ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 completed in 7.74s\n",
      "  Train Loss: 1.0173, Train Accuracy: 0.6357\n",
      "  Eval Loss: 1.0940, Eval Accuracy: 0.6042\n",
      "  Best Accuracy: 0.6097 at epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eedf421faf1487db046de5440dba51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a0b68f9df749a7b69d9fc82b9620fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 completed in 7.72s\n",
      "  Train Loss: 0.9809, Train Accuracy: 0.6471\n",
      "  Eval Loss: 1.0874, Eval Accuracy: 0.6053\n",
      "  Best Accuracy: 0.6097 at epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef877c5244e241de81552c112868d564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a744d060d5b148aba4f32e43b8823268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 completed in 7.65s\n",
      "  Train Loss: 0.9461, Train Accuracy: 0.6606\n",
      "  Eval Loss: 1.0798, Eval Accuracy: 0.6165\n",
      "  Best Accuracy: 0.6165 at epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c431bbddb24a3c9e7fa869529d3f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff48481b65934183a07cd37b5e2ad0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 completed in 7.65s\n",
      "  Train Loss: 0.9102, Train Accuracy: 0.6718\n",
      "  Eval Loss: 1.0379, Eval Accuracy: 0.6284\n",
      "  Best Accuracy: 0.6284 at epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35738597eb314981984264dff944846e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249246c038a64849b5ad4c8a481eb7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 completed in 7.72s\n",
      "  Train Loss: 0.8685, Train Accuracy: 0.6885\n",
      "  Eval Loss: 1.0481, Eval Accuracy: 0.6295\n",
      "  Best Accuracy: 0.6295 at epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf0cfdbd8ad4c39bc914671f624ffca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c8096598894081a23caa83392ab896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 completed in 7.65s\n",
      "  Train Loss: 0.8324, Train Accuracy: 0.7032\n",
      "  Eval Loss: 1.0247, Eval Accuracy: 0.6372\n",
      "  Best Accuracy: 0.6372 at epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f056903e4c74352a2de9a21be73efd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e307f02d9f24fdbafea7e10cdd2d54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 completed in 7.68s\n",
      "  Train Loss: 0.7926, Train Accuracy: 0.7170\n",
      "  Eval Loss: 1.0353, Eval Accuracy: 0.6448\n",
      "  Best Accuracy: 0.6448 at epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b069168a62e41a08803515f598500d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b50640927944925b412221001383d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 completed in 7.73s\n",
      "  Train Loss: 0.7434, Train Accuracy: 0.7332\n",
      "  Eval Loss: 1.0492, Eval Accuracy: 0.6443\n",
      "  Best Accuracy: 0.6448 at epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad398f6a2f44d138ff57ab3b2fa44d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0df63c057134c47a69781c574bb9d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 completed in 7.72s\n",
      "  Train Loss: 0.7024, Train Accuracy: 0.7471\n",
      "  Eval Loss: 1.0911, Eval Accuracy: 0.6354\n",
      "  Best Accuracy: 0.6448 at epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbae10adfe42410dbde96f75cd52028d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8277f01278e14aef83d216b48d788bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 completed in 7.63s\n",
      "  Train Loss: 0.6553, Train Accuracy: 0.7660\n",
      "  Eval Loss: 1.0523, Eval Accuracy: 0.6440\n",
      "  Best Accuracy: 0.6448 at epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3af2b0a570475db7370cca2299e03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976daf14bab548ca91114d9030c6347a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 completed in 7.63s\n",
      "  Train Loss: 0.6047, Train Accuracy: 0.7827\n",
      "  Eval Loss: 1.0732, Eval Accuracy: 0.6479\n",
      "  Best Accuracy: 0.6479 at epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b157dee5bd7c4f41b27641bde44d7f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3fac313b214c4cbfb58ba329765795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 completed in 7.74s\n",
      "  Train Loss: 0.5570, Train Accuracy: 0.8005\n",
      "  Eval Loss: 1.0925, Eval Accuracy: 0.6495\n",
      "  Best Accuracy: 0.6495 at epoch 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11de5c2fbc046dc901c4a4be8c0e063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac332185202487a8e802727d6f1937c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 completed in 7.67s\n",
      "  Train Loss: 0.5072, Train Accuracy: 0.8183\n",
      "  Eval Loss: 1.1373, Eval Accuracy: 0.6521\n",
      "  Best Accuracy: 0.6521 at epoch 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9471db207c649e99832db9a690f82ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f7be3f10704f5b888c7c8b956e4434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 completed in 7.78s\n",
      "  Train Loss: 0.4608, Train Accuracy: 0.8357\n",
      "  Eval Loss: 1.1836, Eval Accuracy: 0.6474\n",
      "  Best Accuracy: 0.6521 at epoch 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e419a23efc4c20952cd7f6d90419bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae6c95a1f1e408bad4e56d80177eb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 completed in 7.80s\n",
      "  Train Loss: 0.4117, Train Accuracy: 0.8528\n",
      "  Eval Loss: 1.2176, Eval Accuracy: 0.6541\n",
      "  Best Accuracy: 0.6541 at epoch 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78039b64ec1742418425eb6a5af22ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef0416898294b86991c024ee59caa34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 completed in 7.78s\n",
      "  Train Loss: 0.3729, Train Accuracy: 0.8658\n",
      "  Eval Loss: 1.2480, Eval Accuracy: 0.6484\n",
      "  Best Accuracy: 0.6541 at epoch 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6837d1ecf99a4b97a52afc42c8292b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deb802ae00a4eb093f528d01478bb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 completed in 7.68s\n",
      "  Train Loss: 0.3369, Train Accuracy: 0.8791\n",
      "  Eval Loss: 1.2915, Eval Accuracy: 0.6521\n",
      "  Best Accuracy: 0.6541 at epoch 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c912638ac64c1e938e29e4da0a1555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8720c7f79946e181a756ad84f77743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 completed in 7.64s\n",
      "  Train Loss: 0.3057, Train Accuracy: 0.8918\n",
      "  Eval Loss: 1.3325, Eval Accuracy: 0.6530\n",
      "  Best Accuracy: 0.6541 at epoch 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f1a0bd3e32452ea5702798aa6318d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c718a5b287ac41f094cbf98cec5f6458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 completed in 7.64s\n",
      "  Train Loss: 0.2849, Train Accuracy: 0.8985\n",
      "  Eval Loss: 1.3331, Eval Accuracy: 0.6560\n",
      "  Best Accuracy: 0.6560 at epoch 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dbee55299b446ab72faae91148fd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d79abddccd4ac9860e04705c091fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 completed in 7.65s\n",
      "  Train Loss: 0.2713, Train Accuracy: 0.9052\n",
      "  Eval Loss: 1.3497, Eval Accuracy: 0.6521\n",
      "  Best Accuracy: 0.6560 at epoch 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085762a6e0ba4f789e2663c6a043fad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2905419c6cf44015af733b6d02cf0877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 completed in 7.88s\n",
      "  Train Loss: 0.2635, Train Accuracy: 0.9084\n",
      "  Eval Loss: 1.3557, Eval Accuracy: 0.6522\n",
      "  Best Accuracy: 0.6560 at epoch 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a431b5414764bbeb4eb60b7a452e44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/30 [Train]:   0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa37f9ede5543509cc537f3d55c87c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/30 [Eval]:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 completed in 7.86s\n",
      "  Train Loss: 0.2574, Train Accuracy: 0.9096\n",
      "  Eval Loss: 1.3568, Eval Accuracy: 0.6532\n",
      "  Best Accuracy: 0.6560 at epoch 27\n",
      "Running final evaluation with feature extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c578401a3ee44994914731d961c3e015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Final Evaluation:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE projection...\n",
      "Computing PCA projection...\n",
      "Computing UMAP projection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating final visualizations...\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Total time: 00:10:50\n",
      "Best accuracy: 0.6560\n",
      "Final F1 score: 0.6515\n",
      "Results in: vit_results/run_20250506_204653\n",
      "============================================================\n",
      "\n",
      "Generated files:\n",
      "  Visualizations:\n",
      "    - training_metrics.png - Learning curves\n",
      "    - lr_schedule.png - Learning rate schedule\n",
      "    - confusion_matrix.png - Confusion matrix\n",
      "    - precision_recall_f1.png - Classification metrics\n",
      "    - tsne_visualization.png - t-SNE feature visualization\n",
      "    - pca_visualization.png - PCA feature visualization\n",
      "    - umap_visualization.png - UMAP feature visualization\n",
      "    - sample_predictions.png - Sample predictions\n",
      "  Text files:\n",
      "    - config.txt - Training configuration\n",
      "    - training_summary.txt - Final results summary\n",
      "    - hardware_summary.txt - Hardware information\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================\n",
    "# 13. MAIN FUNCTION\n",
    "# ==============================\n",
    "def main():\n",
    "    \"\"\"Main entry point for training\"\"\"\n",
    "    \n",
    "    # Set up configuration - modify these values to customize training\n",
    "    config = ViTConfig(\n",
    "        # Model architecture\n",
    "        img_size=32,\n",
    "        patch_size=4,\n",
    "        num_classes=10,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        hidden_dim=384,\n",
    "        mlp_dim=1536,\n",
    "        dropout_rate=0.1,\n",
    "        \n",
    "        # Training parameters\n",
    "        batch_size=128,\n",
    "        num_epochs=30,\n",
    "        learning_rate=3e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=500,\n",
    "        \n",
    "        # Other settings\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Create unique results directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = os.path.join(\"vit_results\", f\"run_{timestamp}\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Print startup information\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VISION TRANSFORMER TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Configuration:\")\n",
    "    print(f\"  - Model: {config.num_layers} layers × {config.hidden_dim} dim\")\n",
    "    print(f\"  - Training: {config.num_epochs} epochs × {config.batch_size} batch size\")\n",
    "    print(f\"  - Learning rate: {config.learning_rate} (warmup: {config.warmup_steps} steps)\")\n",
    "    print(f\"  - Device: {jax.devices()[0]}\")\n",
    "    print(f\"  - Results: {results_dir}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Initialize all components\n",
    "    data_loader = DataLoader(data_dir=\"cifar10_data\")\n",
    "    trainer = Trainer(config, results_dir)\n",
    "    visualizer = Visualizer(results_dir)\n",
    "    \n",
    "    # Save configuration to file\n",
    "    config_path = os.path.join(results_dir, 'config.txt')\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(\"Vision Transformer Configuration\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        for key, value in config.__dict__.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare data\n",
    "        print(\"Loading CIFAR-10 dataset...\")\n",
    "        train_data, train_labels, test_data, test_labels = data_loader.load_datasets()\n",
    "        \n",
    "        # Create learning rate visualization\n",
    "        print(\"Creating visualizations...\")\n",
    "        total_steps = config.num_epochs * (len(train_data) // config.batch_size)\n",
    "        visualizer.plot_learning_rate_schedule(trainer.lr_scheduler or \n",
    "                                             LearningRateScheduler(config, total_steps), \n",
    "                                             total_steps)\n",
    "        \n",
    "        # Run training\n",
    "        print(\"\\nTraining model...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        final_metrics = trainer.train(train_data, train_labels, test_data, test_labels)\n",
    "        \n",
    "        # Create final visualizations\n",
    "        print(\"\\nGenerating final visualizations...\")\n",
    "        visualizer.plot_sample_predictions(\n",
    "            trainer.vit,\n",
    "            trainer.state.params,\n",
    "            test_data,\n",
    "            test_labels\n",
    "        )\n",
    "        \n",
    "        # Calculate total training time\n",
    "        total_time = time.time() - start_time\n",
    "        hours = int(total_time // 3600)\n",
    "        minutes = int((total_time % 3600) // 60)\n",
    "        seconds = int(total_time % 60)\n",
    "        \n",
    "        # Save final results summary\n",
    "        summary_path = os.path.join(results_dir, 'training_summary.txt')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"Vision Transformer Training Summary\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            f.write(f\"Training Time: {hours:02d}:{minutes:02d}:{seconds:02d}\\n\")\n",
    "            f.write(f\"Best Validation Accuracy: {trainer.state.best_accuracy:.4f}\\n\")\n",
    "            f.write(f\"Best Epoch: {trainer.state.best_epoch + 1}\\n\")\n",
    "            f.write(f\"Final Macro F1 Score: {final_metrics['metrics']['macro_f1']:.4f}\\n\")\n",
    "            f.write(f\"Final Macro Precision: {final_metrics['metrics']['macro_precision']:.4f}\\n\")\n",
    "            f.write(f\"Final Macro Recall: {final_metrics['metrics']['macro_recall']:.4f}\\n\")\n",
    "            f.write(f\"\\nResults Directory: {results_dir}\\n\")\n",
    "        \n",
    "        # Save hardware summary\n",
    "        hardware_summary_path = os.path.join(results_dir, 'hardware_summary.txt')\n",
    "        with open(hardware_summary_path, 'w') as f:\n",
    "            f.write(\"Hardware Summary\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            f.write(f\"JAX version: {jax.__version__}\\n\")\n",
    "            f.write(f\"Device used: {jax.devices()[0]}\\n\")\n",
    "            f.write(f\"Number of devices: {jax.device_count()}\\n\")\n",
    "            try:\n",
    "                f.write(f\"Device type: {jax.devices()[0].device_kind}\\n\")\n",
    "            except:\n",
    "                f.write(\"Device type: Unknown\\n\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "        print(f\"Best accuracy: {trainer.state.best_accuracy:.4f}\")\n",
    "        print(f\"Final F1 score: {final_metrics['metrics']['macro_f1']:.4f}\")\n",
    "        print(f\"Results in: {results_dir}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # List generated files\n",
    "        print(\"\\nGenerated files:\")\n",
    "        print(\"  Visualizations:\")\n",
    "        print(\"    - training_metrics.png - Learning curves\")\n",
    "        print(\"    - lr_schedule.png - Learning rate schedule\")\n",
    "        print(\"    - confusion_matrix.png - Confusion matrix\")\n",
    "        print(\"    - precision_recall_f1.png - Classification metrics\")\n",
    "        print(\"    - tsne_visualization.png - t-SNE feature visualization\")\n",
    "        print(\"    - pca_visualization.png - PCA feature visualization\")\n",
    "        print(\"    - umap_visualization.png - UMAP feature visualization\")\n",
    "        print(\"    - sample_predictions.png - Sample predictions\")\n",
    "        print(\"  Text files:\")\n",
    "        print(\"    - config.txt - Training configuration\")\n",
    "        print(\"    - training_summary.txt - Final results summary\")\n",
    "        print(\"    - hardware_summary.txt - Hardware information\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining failed with error: {e}\")\n",
    "        print(f\"Partial results saved to: {results_dir}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exit_code = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0effac-7f62-4b7f-ac53-53222bb69608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
